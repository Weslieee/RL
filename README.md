实战 Demo：AI 如何学会跑出迷宫

纸上得来终觉浅。为了验证刚才推导的 Q-Learning 公式，我们需要构建一个标准的强化学习仿真环境。

在这个代码演示中，我们用 Python 搭建了一个5*5 的数字迷宫，设定如下：

地图：一个5*5的黑暗网格，没有任何路标或导航。

目标：控制机器人从左上角出发，到达右下角的终点。

动作：只能选择 上、下、左、右 四个方向移动。

反馈：只有走到终点，环境才会给出唯一的奖励+1；在其他任何格子里徘徊，奖励统统是0。

接下来的代码，本质上就是在模拟一个“感知、决策 、反馈 、修正”的时间闭环。每一次循环，都是智能体与这个数字世界的一次博弈。



让我们看看它是如何从一无所知，进化到从容通关的。



1. 核心逻辑拆解：算法的三个支柱

代码中真正驱动智能体进行强化学习的核心算法，仅由以下三个关键模块构成。它们对应了 Q-Learning 的标准数学定义：



① 价值载体：Q-Table 的初始化

我们在代码中维护了一个 pandas 数据框、作为 Q 值表。25行对应5*5迷宫中的每一个坐标，4列对应上、下、左、右四个动作。



所有 Q 值初始化为 0.0。这意味着在训练开始前，智能体对环境中任何状态-动作对的潜在价值预估均为零，没有任何先验知识。


② 决策策略：epsilon-greedy (贪婪策略)

智能体如何选择动作？代码实现了经典的epsilon-greedy 策略，以平衡“探索”与“利用”：

探索：以小概率（如epsilon=0.1）随机选择动作。这确保了智能体不会陷入局部最优，有机会遍历未知的状态。

利用：以大概率（如1-epsilon=0.9）选择当前 Q 表中分值最高的动作。这是基于现有经验做出的最优决策。



③ 核心算法：贝尔曼方程的迭代更新

这是整个程序的最核心部分，即每一次交互后的Q 值修正。代码逻辑严格遵循 Q-Learning 的更新公式：

举个例子：

假设我们在5*5的地图中，终点在 [4, 4]。 当前智能体站在终点左边一格 [4, 3]，它决定向右走一步。



1. 行动前（旧记忆）

智能体查表：Q( [4, 3], 'right' )，因为是第一次探索到这里，表格里记录的数是 0，智能体觉得这一步平平无奇，没啥好处。



2. 行动中（拿奖励）

智能体执行了 right，移动到了 [4, 4]。此时环境反馈Reward+1（到终点）此时游戏结束，没有下一步了（下一步价值为 0）



3. 更新表格

现在开始套用公式更新Q( [4, 3], 'right' )的值：

现实目标：1(奖励) + 0(没有下一步) = 1.0

误差（现实 - Q_old)：1.0(现实) - 0 (表中原来的数) = 1.0

修正幅度：0.1(学习率) *1.0(误差) = 0.1

更新后的Q值：0.0 + 0.1 = 0.1



4. 结果

更新完后，Q 表里Q( [4, 3], 'right' )变成了0.1，这意味着什么？下次智能体再路过 [4, 3] 时，它会发现：

向上/下/左走：Q=0；向右走：Q=0.1(最高)。

于是，它会毫不犹豫地选择向右。这就是“经验”被写入“大脑”的全过程。

2. 演进过程：从“无头苍蝇”到“极速通关”

运行仿真程序后，我们可以直观地观察到 Q-Learning 算法的收敛特性。

请重点关注控制面板右侧的 STEPS（回合步数） 指标。这一量化数据的变化趋势，客观地反映了智能体对环境认知的深化过程。整个训练周期可清晰地划分为三个典型的行为阶段：



第一阶段：随机探索期 ( Ep 1~10)
由于 Q-Table 初始化为全零，智能体缺乏先验知识，主要依赖随机概率进行盲目试错。其行为表现为高熵值的随机游走，单局步数极不稳定且通常超过 50 步，表明智能体尚未建立有效的状态映射，决策处于无序状态。



第二阶段：策略提升期 ( Ep 11~30)
伴随触达终点触发的奖励信号，贝尔曼方程将价值反向传播，在状态空间中形成初步的价值梯度。受此引导，智能体决策开始具备方向性，从随机策略向贪婪策略平滑过渡，步数显著下降并收敛至 20~30 步区间，路径冗余大幅减少。



第三阶段：收敛 (Ep 31~50)
随着 TD 误差趋近于零，Q 值达到稳定状态，智能体完全掌握了环境的最优策略。实验数据显示，回合步数最终恒定在 8 步，这一数值精确匹配了网格世界中曼哈顿距离的理论下限，客观验证了算法已收敛至全局最优解。



