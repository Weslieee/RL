import numpy as np
import pandas as pd
import time
import tkinter as tk
from tkinter import ttk # ç”¨äºæ›´å¥½çœ‹çš„æ§ä»¶


CFG = {
    'ROWS': 5,
    'COLS': 5,
    'UNIT': 90,           # æ ¼å­ç¨å¾®å¤§ä¸€ç‚¹
    'BG': '#1e1e2e',      # å…¨å±€èƒŒæ™¯ (æ·±ç©ºç°è“)
    'PANEL': '#2b2b40',   # ä¾§è¾¹æ èƒŒæ™¯
    'GRID': '#313244',    # æ ¼å­èƒŒæ™¯ (æš—è‰²)
    'ACCENT': '#89b4fa',  # å¼ºè°ƒè‰² (äº®è“) - å¯¹åº”ç½‘é¡µ Agent é¢œè‰²
    'TARGET': '#f9e2af',  # ç»ˆç‚¹é¢œè‰² (æ·¡é»„)
    'TEXT': '#cdd6f4',    # æ–‡å­—é¢œè‰² (äº‘ç™½)
    'FONT_MAIN': ('Segoe UI', 12),
    'FONT_BOLD': ('Segoe UI', 12, 'bold'),
    'FONT_EMOJI': ('Segoe UI Emoji', 30), # ä¸“é—¨æ˜¾ç¤º Emoji
}

ACTIONS = ['up', 'down', 'left', 'right']
Q_TABLE = pd.DataFrame(columns=ACTIONS, dtype=np.float64)

# --- 2. ç•Œé¢æ ¸å¿ƒç±» ---
class CyberMaze(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title('ğŸ¤– Q-Learning è¿·å®«æµ‹è¯•')
        self.configure(bg=CFG['BG'])
        
        # çª—å£å±…ä¸­è®¡ç®—
        w = CFG['COLS'] * CFG['UNIT'] + 260 # 260æ˜¯ä¾§è¾¹æ å®½åº¦
        h = CFG['ROWS'] * CFG['UNIT'] + 40  # 40æ˜¯ç•™ç™½
        self.geometry(f'{w}x{h}')
        self.resizable(False, False)

        self.setup_ui()
        
    def setup_ui(self):
        # --- å·¦ä¾§ï¼šæ¸¸æˆåœ°å›¾ ---
        # ä½¿ç”¨ Frame åŒ…è£¹ Canvas å®ç°è¾¹è·
        game_frame = tk.Frame(self, bg=CFG['BG'], padx=20, pady=20)
        game_frame.pack(side='left')
        
        self.canvas = tk.Canvas(game_frame, bg=CFG['BG'],
                                height=CFG['ROWS'] * CFG['UNIT'],
                                width=CFG['COLS'] * CFG['UNIT'],
                                highlightthickness=0) # å»é™¤ä¸‘é™‹çš„é»˜è®¤è¾¹æ¡†
        self.canvas.pack()

        # ç”»èƒŒæ™¯ç½‘æ ¼ (ç”¨çŸ©å½¢ä»£æ›¿çº¿æ¡ï¼Œæ¨¡ä»¿ CSS Grid gap æ•ˆæœ)
        self.cells = {} # å­˜å‚¨æ ¼å­åæ ‡
        for r in range(CFG['ROWS']):
            for c in range(CFG['COLS']):
                x0, y0 = c * CFG['UNIT'], r * CFG['UNIT']
                x1, y1 = x0 + CFG['UNIT'], y0 + CFG['UNIT']
                
                # ç»˜åˆ¶æ ¼å­åº•è‰² (ç•™å‡º 4px é—´éš™æ¨¡æ‹Ÿ gap)
                gap = 4
                self.canvas.create_rectangle(
                    x0 + gap, y0 + gap, x1 - gap, y1 - gap,
                    fill=CFG['GRID'], outline=''
                )

        # --- å³ä¾§ï¼šæ§åˆ¶é¢æ¿ ---
        self.panel = tk.Frame(self, bg=CFG['PANEL'], width=240)
        self.panel.pack(side='right', fill='y', ipadx=20)
        self.panel.pack_propagate(False) # å›ºå®šå®½åº¦

        # æ ‡é¢˜
        tk.Label(self.panel, text="Reinforcement\nLearning", font=('Impact', 24),
                 bg=CFG['PANEL'], fg=CFG['ACCENT'], justify='left').pack(pady=(30, 20), anchor='w')

        # æ•°æ®æ˜¾ç¤º
        self.var_ep = tk.StringVar(value="EPISODE: 0")
        self.var_step = tk.StringVar(value="STEPS: 0")
        
        self._create_stat_card("å±€æ•°ç»Ÿè®¡", self.var_ep)
        self._create_stat_card("å½“å‰æ­¥æ•°", self.var_step)

        # é€Ÿåº¦æ§åˆ¶æ»‘å—
        tk.Label(self.panel, text="SIMULATION SPEED", font=('Arial', 8, 'bold'),
                 bg=CFG['PANEL'], fg='#6c7086').pack(anchor='w', pady=(30, 5))
        
        self.scale_speed = tk.Scale(self.panel, from_=0.01, to=0.5, resolution=0.01,
                                    orient='horizontal', length=180,
                                    bg=CFG['PANEL'], fg=CFG['TEXT'], 
                                    troughcolor=CFG['BG'], highlightthickness=0,
                                    label="", showvalue=0)
        self.scale_speed.set(0.1) # é»˜è®¤é€Ÿåº¦
        self.scale_speed.pack(anchor='w')
        
        # åº•éƒ¨çŠ¶æ€
        self.lbl_status = tk.Label(self.panel, text="READY", font=CFG['FONT_BOLD'],
                                   bg=CFG['PANEL'], fg='#a6adc8')
        self.lbl_status.pack(side='bottom', pady=30)

        # åˆå§‹åŒ–è§’è‰²
        self.reset_agent_target()

    def _create_stat_card(self, title, var):
        # ç®€å•çš„å¡ç‰‡æ ·å¼
        frame = tk.Frame(self.panel, bg=CFG['BG'], pady=10, padx=10)
        frame.pack(fill='x', pady=5)
        tk.Label(frame, text=title, font=('Arial', 8), bg=CFG['BG'], fg='#6c7086').pack(anchor='w')
        tk.Label(frame, textvariable=var, font=('Arial', 14, 'bold'), bg=CFG['BG'], fg=CFG['TEXT']).pack(anchor='w')

    def reset_agent_target(self):
        self.canvas.delete("agent")
        self.canvas.delete("target")
        
        # ç»˜åˆ¶ç»ˆç‚¹ ğŸ’
        tx, ty = CFG['COLS']-1, CFG['ROWS']-1
        cx, cy = self._get_center(tx, ty)
        # å‘å…‰èƒŒæ™¯
        self.canvas.create_oval(cx-30, cy-30, cx+30, cy+30, fill=CFG['TARGET'], outline='', tags="target")
        # Emoji
        self.canvas.create_text(cx, cy, text="ğŸ’", font=CFG['FONT_EMOJI'], tags="target")

        # ç»˜åˆ¶ä¸»è§’ ğŸ¤– (åˆå§‹åœ¨ 0,0)
        self.agent_pos = [0, 0]
        self.draw_agent(0, 0)

    def draw_agent(self, r, c):
        self.canvas.delete("agent")
        cx, cy = self._get_center(c, r)
        # ç»˜åˆ¶åœ†è§’çŸ©å½¢èƒŒæ™¯ (ç”¨ oval æ¨¡æ‹Ÿåœ†å½¢å…‰æ™•)
        self.canvas.create_rectangle(
            c*CFG['UNIT']+8, r*CFG['UNIT']+8, 
            (c+1)*CFG['UNIT']-8, (r+1)*CFG['UNIT']-8,
            fill=CFG['ACCENT'], outline='', tags="agent"
        )
        self.canvas.create_text(cx, cy, text="ğŸ¤–", font=CFG['FONT_EMOJI'], tags="agent")

    def _get_center(self, c, r):
        return c * CFG['UNIT'] + CFG['UNIT']/2, r * CFG['UNIT'] + CFG['UNIT']/2

    def update_view(self, ep, step, done=False):
        self.var_ep.set(f"EPISODE: {ep+1}")
        self.var_step.set(f"STEPS: {step}")
        if done:
            self.lbl_status.config(text="ğŸ‰ SUCCESS!", fg='#a6e3a1') # ç»¿è‰²
        else:
            self.lbl_status.config(text="TRAINING...", fg='#f9e2af') # é»„è‰²
        self.update()

# --- 3. ç®—æ³•é€»è¾‘ (Q-Learning) ---
def check_state(state):
    state_str = str(state)
    if state_str not in Q_TABLE.index:
        Q_TABLE.loc[state_str] = [0.0] * 4

def choose_action(state):
    check_state(state)
    if np.random.uniform() < 0.1 or (Q_TABLE.loc[str(state)] == 0).all():
        return np.random.choice(ACTIONS)
    return Q_TABLE.loc[str(state)].idxmax()

def run_game():
    env = CyberMaze()
    # å»¶è¿Ÿå¯åŠ¨ï¼Œç»™ UI æ¸²æŸ“æ—¶é—´
    env.after(1000, lambda: train_loop(env))
    env.mainloop()

def train_loop(env):
    for episode in range(50): # è®­ç»ƒ50è½®
        state = [0, 0]
        env.reset_agent_target()
        is_terminated = False
        step = 0
        
        while not is_terminated:
            # 1. è·å–æ»‘å—é€Ÿåº¦
            sleep_t = env.scale_speed.get()
            time.sleep(sleep_t)
            
            # 2. ç®—æ³•å†³ç­–
            action = choose_action(state)
            
            # 3. ç§»åŠ¨é€»è¾‘
            next_state = state.copy()
            if action == 'up':    next_state[0] = max(0, state[0]-1)
            elif action == 'down':  next_state[0] = min(CFG['ROWS']-1, state[0]+1)
            elif action == 'left':  next_state[1] = max(0, state[1]-1)
            elif action == 'right': next_state[1] = min(CFG['COLS']-1, state[1]+1)
            
            # 4. å¥–åŠ±åˆ¤æ–­
            reward = 0
            if next_state == [CFG['ROWS']-1, CFG['COLS']-1]:
                reward = 1
                is_terminated = True
            
            # 5. æ›´æ–° Q è¡¨
            check_state(next_state)
            q_predict = Q_TABLE.loc[str(state), action]
            if is_terminated:
                q_target = reward
            else:
                q_target = reward + 0.9 * Q_TABLE.loc[str(next_state)].max()
            
            Q_TABLE.loc[str(state), action] += 0.1 * (q_target - q_predict)
            
            # 6. UI æ›´æ–°
            state = next_state
            env.draw_agent(state[0], state[1])
            env.update_view(episode, step, is_terminated)
            step += 1
        
        # é€šå…³åç¨å¾®åœé¡¿
        time.sleep(0.5)

if __name__ == "__main__":
    run_game()